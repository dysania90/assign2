server netscapecommerce112  date tuesday 26nov96 000613 gmt  lastmodified thursday 15jun95 004049 gmt  contentlength 18834  contenttype texthtml              theory of computation group                      as one of the worlds largest groups of theoretical computer  researchers the laboratory for computer science lcs  pursues nearly every major area of computer technology our  interests range from basic mathematical theory  such as  computational geometry complexity theory and  numbertheoretic algorithms  to theoretical work on the  foundations of electronic circuitry communications  biology cryptography and computer architectures      an important goal of theoretical computer science is to  create formal models of computation then explore what is  possible within those models the results not only deepen  our understanding of the basics of computer science but  also alter its practice through more efficient algorithms  novel architectures and a better understanding of a  programs meaning while many of these models reflect  recent technological advances  parallel or distributed  computing for example  work also is performed on such  traditional models as finite automata and ordinary  sequential computers           parallel algorithms   efficient algorithms  scientific computing  computational biology  machine learning   computational complexity  cryptographic protocols  program semantics   distributed computing        mit is the worlds leader in parallel algorithms and  architectures we thus work closely with architects and  systems designers to create the next generation of parallel  supercomputers faculty and students interact with such  leading companies as thinking machines and ibm to design  and analyze communication networks parallel computation  models efficient parallel algorithms for various  applications and methods for making largescale parallel  machines more faulttolerant      not surprisingly we are deeply involved in the design and  use of the forthcoming information highway efficient  networkbased communications in fact is one of the most  important and exciting challenges facing theory  researchers                              tom leighton professor of applied mathematics  parallel algorithms   michel goemans assistant professor of applied mathematics efficient algorithms               lcs also vigorously researches efficient algorithms for  sequential computers surprisingly perhaps improved  algorithms for wellknown problems continue to be  discovered and new theoretical problems often arise as  spinoffs from advances in computer technology some of our  work focuses on algorithms for graph problems  computational geometry numbertheoretic problems and the  laying out and routing of vlsi circuitry other recent  projects include online algorithms which do not know all  the data in advance randomized algorithms which use  random numbers to aid decisionmaking and approximation  algorithms which are guaranteed to find nearoptimum  solutions      many fundamental problems that provide insight into the  design and analysis of efficient algorithms lie in the area  of combinatorial optimization we recently have seen a  surge of exciting developments in approximation algorithms  for difficult optimization problems and lcs is a leader in  obtaining novel and general techniques for designing such  algorithms we have developed improved approximation  algorithms for a variety of problems including those  related to multicommodity flow and network design as well  as more specific problems such as the graph bisection  problem or the maximum cut problem                              alan edelman assistant professor of  applied mathematics  scientific computing   david r karger assistant professor of computer science  and engineering algorithms            largely as a result of rapid advances in parallel computing  technologyscientific computing has become one of  computer sciences most active areas this  interdisciplinary area bridges numerical analysis linear  algebra computer architecture program analysis and  optimization software engineering scientific  visualization and various scientific applications      problems in scientific computing often strain the resources  of modern parallel machines compelling us to advance new  tools and ideas lcs researchers have pioneered the  adaptation of algorithms for the special needs of these  scientific applications      scientific computing involves various research topics in  theoretical computer science such as finite element and  finite difference mesh generation sparse and dense matrix  computations and the solution of largescale linear  systems  some of these problems can be translated into or  approximated by combinatorial and geometric problems  including network optimization communication network  topology emulation graph embedding parallel machine  scheduling dynamic load balancing geometric modeling and  triangulations    a fundamental issue in parallel scientific computing is  mesh partitioning in which a large mesh is divided into a  given number of pieces of roughly equal weights so that the  boundary is small efficient partitioning is vital to  balance the load and reduce communication in parallel  solutions of sparse linear systems it is also useful in  parallel emulation of computational meshes on hypercube and  butterfly architectures and outofcore algorithms for  iterative relaxation      computational biology represents another new and  exciting research area accordingly one of our goals is to  expand the computational toolkit that is available for  numerous biological problems      computer science for example helps make sense of the vast  amount of information now being compiled for the human  genome project such as dna and aminoacid sequence data  one intramiteffort has drawn on the resources of lcs the  whitehead institute and the biology and mathematics  departments specific research areas include computational  approaches to protein folding physical and genetic  mapping virus shell assembly aids theories and sequence  homology and alignment        yet another illustration of computational biology relates  to the socalled grand challenge associated with protein  folding  that is the determination of how a protein will  fold threedimensionally when only its aminoacid sequence  is known an important first step in answering this  question is a solution to the motif recognition problem  given a known 3d structure or motif researchers must  determine whether the fold occurs in an unknown sequence of  amino acids and if so in which positions techniques from  theoretical computer science have been particularly  effective in solving such problems                                  ronald l rivest edwin sibley webster professor of computer science  and engineering  associate director lcs machine learning    bonnie a berger assistant professor of mathematics  computational biology               on another front researchers in machine learning study  computers ability to learn from experience the results  of our research have stimulated various formalizations that  address a range of issues in psychology artificial  intelligence pattern recognition and neurobiology some  recent research themes include the inference of finite  automata learning in the presence of noise learning an  unknown environment piecemeal by exploration learning of  manifest systems in which relevant variables are often  but not always visible to the learner and models of  teaching      in general the groups research is positive in nature  in that we strive to develop provably efficient learning  algorithms with potentially practical application in some  cases however the research leads to equally useful  negative results by identifying the limits of what is  ultimately learnable      another major theme is the development of new models of  learning that provide better theoretical formulations of  realworld learning situations and the appropriate  algorithms one example is to learn a concept defined by  boolean formulae from examples of that concept another is  to infer the structure of a finitestate system by  examining the systems inputoutput behavior statistical  techniques are needed to determine how much data are needed  for a given problem complexity theory helps assess the  difficulty of computing the desired answer from the data      machinelearning research is generally theoretical in  nature although some is experimental and involves the  careful specification of models of learning and the precise  specification and analysis of learning algorithms we use a  wide range of models to capture different aspects of  technical and philosophical relevance such as learning  from noisy data learning hierarchically structured  concepts learning with neural nets learning via different  output representations learning to represent a system  containing hidden state variables and trading off  simplicity of hypothesis with quality of fit to the data                                  shafrira goldwasser professor of  electrical engineering  and  computer science cryptographic protocols    silvio micali professor of  electrical engineering   and  computer science cryptographic protocols               cryptography is another important area of research  within lcs in its simplest and most ancient form  cryptography relates to secret communication cast in the  framework of complexity theory the sender the recipient  and the adversary are computationally bounded machines an  encryption system is deemed secure when it is  computationally unfeasible for an adversary to obtain  information from their encodings since proving nontrivial  lower bounds on the complexity of npcomplete problems is  not within the current state of the art proof of security  must show that any method for compromising security can be  transformed into an efficient algorithm for a problem such  as factoring integers which is generally believed to be  intractable      achieving privacy is only one area of cryptography  research another is the design of protocols for  authentication certified electronic mail and contract  signing between two or more mutually suspicious parties in  general the goal is to perform an arbitrary distributed  computation among many processors each containing some  portion of the input each processor is connected in a  network such that no processor reveals any information  other than that which is intended      protocol research has led to a complexity theory of the  amount of knowledge that must be released in order for one  processor to prove a fact to another processor  the  theory of zeroknowledge proofs generating pseudorandom  numbers and functions is another important field  randomness is here defined with respect to a specific  model of computation and a specific level of computational  resources      lcs researchers have contributed to virtually all  cryptographic inventions of the past decade including the  invention of the first publickey cryptosystem  probabilistic cryptosystems and the invention of  zeroknowledge proofs                                  michael sipser professor of applied mathematics computational complexity theory    mauricio karchmer assistant professor of mathematics computational complexity theory               lcs also enjoys a traditional leadership role in  computation complexity theory one of the prime goals  in this field is to devise and study natural schemes for  classifying problems according to their computational  difficulty then place familiar and important problems  within the appropriate scheme      one familiar example is the problem of factoring large  integers  that is finding all the prime numbers that  divide the integer evenly the exercise is not only  theoretically interesting but also is relevant to  cryptography the brute force method of searching for  prime factors one by one is too slow to be useful while  better algorithms are known determining the intrinsic  difficulty of the factoring problem is one of complexity  theorys many exciting questions      lcs researchers were the first to show that there are  problems of high intrinsic complexity now we are  investigating the complexity of problems akin to factoring  by studying the power of weak computational models such as  branching programs and monotone circuits of bounded depth  these formally constrained models are easier to analyze and  thus may help us better understand the standard models in  closely related work we are studying the power of  probabilistic computation parallelism randomness and  pseudorandomness interactive proof systems and other  basic computing concepts                              albert r meyer hitachi america professor  of engineering program semantics and logic    peter elias professor  emeritus and senior lecturer information theory               elsewhere within lcs researchers into the theory of  programming semantics and logic aim to provide clear  mathematical foundations and reliable reasoning principles  that conform to the robust functional metaphor programmers  use to design describe and justify their programs  programming routinely unites high abstraction and pragmatic  design and includes the declaration of procedures  functions data types processes and objects the  researchers objective is to lay a solid foundation for  this task      computer scientists notion of a function however may  depend on when and in what context it is evaluated that  contrasts with the mathematicians classical notion of a  function bridging this conceptual gap involves elements of  algebra modal and intuitionistic logic and category  complexity computability proof and type theory  all  applied to programminglanguage design compiler  construction and program optimization this work is being  extended to the study of specifying the meaning and  verification of the properties of parallel and distributed  processes                             nancy lynch professor of electrical engineering  and computer science distributed computing    baruch awerbuch research scientist distributed computing               distributed computation theory is designed to clarify  the basic capabilities and limitations of concurrent and  distributed computing systems research results include new  algorithms and their analysis impossibility results  formal concurrentsystems models and models and techniques  for proving correctness of concurrent algorithms problems  typically include getting the nonfailing processors to  agree synchronizing nonfailing processors faulttolerant  compiling and routing resource allocation sharing access  to data and graphtheoretic problems such as doing a  breadthfirst search or finding minimumcost spanning  trees      a basic problem in faulttolerant computing is to cause  processors to agree among themselves about the value of a  data item say or about a common course of action while  this is a simple exercise in the absence of faults it may  be impossible when faults are present individual  processors do not have reliable knowledge about the states  of other processors our work has led to many interesting  algorithms and impossibility results that demonstrate  conditions under which consensus can or cannot be achieved      an important lcs project related to network protocols has  been the development of a series of efficient algorithmic  transformers the result of this project is the  compilation of protocols that were designed for a  relatively simple network model into protocols that run in  a more complex but more realistic environment      lcs has developed an important formalism  the  inputoutput automaton model a basic mathematical model  for concurrent and distributed systems and their  components this simplestate machine model helps describe  interactions between a concurrent system and its  environment the model not only verifies the correctness of  algorithms but also helps find and fix serious gaps in some  basic existing algorithms  the construction of  multiwriter atomic registers for example                  
